# Issue 1: NewAgent 开发文档（更新版）

本文档描述当前版本 `NewAgent`（[new_agent.py](/agents/new_agent.py)）的实现方法、关键设计选择与调参思路，并记录我们目前的评测结论。

## 1. 当前效果与结论

对战基线为课程提供的 `BasicAgent`。

我们目前测了 120 局：`NewAgent` 胜 102 局，胜率稳定在 85%。

```bash
最终结果： {'AGENT_A_WIN': 18, 'AGENT_B_WIN': 102, 'SAME': 0, 'AGENT_A_SCORE': 18.0, 'AGENT_B_SCORE': 102.0}
```

由于胜率未达到88%，因此我们不做与`BasicAgentPro`的对战尝试。

## 2. 总体方法概览

当前 `NewAgent` 的核心是：

1. 用“几何启发式”构造一批质量不同、覆盖不同风险偏好的候选动作（包含进攻候选、解球候选、防守候选、少量随机候选）。
2. 对候选动作进行多次“带噪声的物理模拟”，使用 reward 对结果打分，并额外加入“犯规/黑8早进/无碰库”等规则风险惩罚。
3. 用 UCB（类似多臂老虎机/MCTS 选择）在候选动作间分配模拟预算：既利用高分动作，也探索不确定动作。
4. 最终输出平均收益最高的动作；若全体收益都偏低，则退回到安全球策略。

这套框架的目标不是只追求“理论最容易进球”，而是尽量让策略在存在出杆误差、存在复杂碰撞的情况下仍然稳健（减少送分与犯规）。

## 3. 关键模块与实现细节

### 3.1 动作表示与出杆误差建模

`NewAgent` 输出动作字典：

```python
{
  "V0": float,    # 出杆速度
  "phi": float,   # 水平角度（度）
  "theta": float, # 抬杆角（度）
  "a": float,     # 横向塞（归一化）
  "b": float      # 纵向塞（归一化）
}
```

实现上有两步保证动作可用且更贴近实际：

- `_clip_action`：把动作裁剪到合法范围，避免极端输入导致模拟不稳定
- `_add_noise`：对动作参数加入高斯噪声，再裁剪（模拟环境/执行误差）

对应代码见 [new_agent.py:L44-L62](/agents/new_agent.py#L44-L62)。

### 3.2 进攻候选：幽灵球 + 过滤 + 粗概率
在最初的设计中，我希望直接使用幽灵球进攻策略来获得一个较好的结果，但是不论我怎么优化，胜率就卡在75%无法继续提升。因此，在后续策略设计中，我对进攻策略做出如下调整。

我们仍然保留并强化了幽灵球进攻候选生成，但不再“只靠几何打分直接出杆”，而是把它当作候选动作的一部分，再交给后续的物理仿真与 UCB 分配预算。

流程：

- `_calculate_ghost_ball_pos` 计算幽灵球位置（目标球沿目标-袋口方向入袋）
- 对“目标球→袋口”和“母球→幽灵球”两条路径做碰撞检测 `_check_collision_path`
- 剔除过薄球（切球角阈值）
- 使用 `_estimate_pot_prob` 做粗略进球概率估计，把明显低概率的候选剪枝
- 为每个高分候选做角度与速度的小扰动扩增（提高鲁棒性）

对应代码见：

- 幽灵球： [new_agent.py:L64-L104](/agents/new_agent.py#L64-L104)
- 路径碰撞检测： [new_agent.py:L105-L155](/agents/new_agent.py#L105-L155)
- 候选生成： [new_agent.py:L392-L479](/agents/new_agent.py#L392-L479)

### 3.3 广覆盖候选：直瞄目标的“解局/保底”动作集

仅靠“可进球的幽灵球候选”会在以下局面退化：

- 被斯诺克/遮挡导致没有可行幽灵球路径
- 对局中后期可进球路径存在，但对出杆误差非常敏感

因此增加 `_generate_broad_actions`：

- 直接朝目标球方向生成一组 `phi` 偏移与 `V0` 缩放的动作网格
- 目的不是保证进球，而是尽量“先合法触球、能解局、能把局面打活”

对应代码见 [new_agent.py:L481-L513](/agents/new_agent.py#L481-L513)。

### 3.4 防守策略：轻推最近目标球，降低犯规风险

当候选整体收益偏低或候选集为空时，`NewAgent` 会使用 `_safety_action`：

- 找最近的合法目标球
- 中杆轻推，速度与距离成比例（保证大概率能接触目标球并避免白球乱飞）

对应代码见 [new_agent.py:L235-L275](/agents/new_agent.py#L235-L275)。

### 3.5 开球检测与开球动作

开球是分布非常特殊的局面，普通进攻/解局候选不稳定，因此单独识别并生成开球候选：

- `_is_break_state`：用“球堆密集程度 + 母球与球堆距离”粗判是否开球局面
- `_break_action`：以球堆质心为目标方向、给定较大 `V0` 的开球动作

对应代码见：

- 开球识别： [new_agent.py:L277-L298](/agents/new_agent.py#L277-L298)
- 开球动作： [new_agent.py:L299-L312](/agents/new_agent.py#L299-L312)

### 3.6 物理模拟与超时保护

每次评估候选动作都进行物理模拟：

- 深拷贝 `balls` 和 `table`，避免污染真实环境状态
- 对动作加噪声（贴近实战误差）
- 使用 `simulate_with_timeout(..., timeout=3)`，避免单次模拟卡死拖垮整局决策

对应代码见 [new_agent.py:L515-L529](/agents/new_agent.py#L515-L529)。

### 3.7 评价函数：reward + 规则风险惩罚 + 走位评估

我们使用三类信号融合：

1. 课程/工程已有的 `analyze_shot_for_reward`（来自 `basic_agent.py`），用于评估进球、犯规、局面变化等综合收益
2. `_extra_penalty`：从模拟的 `shot.events` 中解析“首次碰撞对象、是否碰库、是否黑8早进、是否白球落袋”等信息，显式加大规则风险惩罚，减少“看似能进但高概率送分”的选择
3. `evaluate_position`：1-step 的“母球是否容易打到下一颗球”的位置质量评分，鼓励走位与连续得分

对应代码见：

- 走位评估： [new_agent.py:L156-L233](/agents/new_agent.py#L156-L233)
- 额外规则惩罚： [new_agent.py:L321-L390](/agents/new_agent.py#L321-L390)
- 在决策中的融合： [new_agent.py:L586-L596](/agents/new_agent.py#L586-L596)

### 3.8 决策主循环：UCB 分配预算（候选动作上的轻量 MCTS）

决策主循环见 [new_agent.py:L531-L607](/agents/new_agent.py#L531-L607)，关键点：

- 候选动作集来自“开球候选 / 广覆盖候选 / 幽灵球候选 / 安全球 / 少量随机”
- 预算 `sims` 根据局面变化（开球/终局/中局）设置不同值
- 对每个动作维护 `N[i]`（访问次数）与 `Q[i]`（累计收益）
- 用 UCB 选择要模拟的动作：
  - 前 `n_candidates` 次先让每个候选至少被模拟一次
  - 之后按照 UCB 值选择动作（探索与利用平衡）
- reward 做线性归一化后累加，避免极端负分把数值尺度拉得过大导致选择不稳定
- 若最优动作平均收益仍过低，回退 `_safety_action`

## 4. 调参与迭代思路（为什么这么做）

### 4.1 为什么不只用“幽灵球进攻”

单纯幽灵球策略有两个典型问题：

- 很多局面没有“理论可进”的直达路径（被遮挡/被斯诺克）
- 即便理论可进，真实环境存在出杆噪声，薄球/长距离对误差极敏感，容易造成犯规或把机会送给对手

因此我们把幽灵球变成“候选生成器之一”，并增加广覆盖/防守候选，通过物理仿真去筛掉风险方案。

### 4.2 为什么要加噪声模拟

如果只在理想出杆下选择动作，现实执行时会出现“选出来的方案对误差极敏感”的问题。把噪声放进评估环节，相当于做了一个简单的鲁棒优化：优先选择“在一簇扰动下仍然有不错期望收益”的动作。

### 4.3 为什么要显式惩罚犯规与黑8早进

纯 reward 往往不足以覆盖“规则风险”这一类事件（尤其是一些边界情况）。因此 `_extra_penalty` 显式把：

- 白球落袋
- 黑8提前落袋
- 首次碰撞不合法
- 未进球且未碰库

都作为强惩罚项，目标是把“看起来收益很高但一旦犯规直接判负/送手”这种动作压下去。

## 5. 复现实验与验证方式

### 5.1 单元测试

运行单元测试：

```bash
python -m unittest -q
```

### 5.2 对战评测（与 BasicAgent）

在 [evaluate.py](/evaluate.py) 中保持测评逻辑不变，只设置对局数 `n_games = 120`，并确保对战为：

```python
agent_a, agent_b = BasicAgent(), NewAgent()
```

然后运行：

```bash
python evaluate.py
```

## 6. 仍可继续优化的方向

- 更“显式”的 safety：将防守质量做成单独评分目标（例如留斯诺克概率/母球停位风险）
- 更强的走位：把 `evaluate_position` 扩展为 2-step（自己下一杆 + 对手下一杆）近似
- 更细的进球概率模型：替换 `_estimate_pot_prob` 的粗指数模型，引入袋口几何、贴库、障碍密度等特征
